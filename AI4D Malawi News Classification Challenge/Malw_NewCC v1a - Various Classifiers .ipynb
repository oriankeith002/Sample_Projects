{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "import pickle \n",
    "import seaborn as sns\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('Train.csv')\n",
    "test = pd.read_csv('Test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['News_Length'] = train['Text'].str.len()\n",
    "print(train['News_length'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train['News_Length']).set_title('News Length Distribution');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning:\n",
    "\n",
    "- removing the special characters\n",
    "- removing the punctuations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Text'] = train['Text'].str.lower()\n",
    "train['Text'] = train['Text'].str.replace(\"[^a-zA-Z]\",\" \")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "word = []\n",
    "\n",
    "for i in train['Text']:  #loops over the text \n",
    "    word.append(word_tokenize(i))''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a stopwords list \n",
    "\n",
    "filter_sent  = [] \n",
    "\n",
    "stopwords_mal = [\" \", \" \"]\n",
    "\n",
    "for i in word:\n",
    "    l = []\n",
    "    \n",
    "    for j in i:\n",
    "        if j not in stopwords_mal:\n",
    "            l.append(j)\n",
    "    fiter_sent.append(' '.join(l))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Text'] = filter_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Clouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(words): # takes input as string\n",
    "    \n",
    "    wordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110).generate(words)\n",
    "    plt.figure(figsize = (10, 7))\n",
    "    plt.imshow(wordcloud, interpolation = \"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = train[train.label == \"YOUR LABEL HERE\"]\n",
    "text = subset.Text.values # list of sentences\n",
    "words  = \" \".join(text) # convert list of sentences into a paragraph of sentences\n",
    "create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = train[train.label == \"YOUR LABEL HERE\"]\n",
    "text = subset.Text.values # list of sentences\n",
    "words  = \" \".join(text) # convert list of sentences into a paragraph of sentences\n",
    "create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = train[train.label == \"YOUR LABEL HERE\"]\n",
    "text = subset.Text.values # list of sentences\n",
    "words  = \" \".join(text) # convert list of sentences into a paragraph of sentences\n",
    "create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = train[train.label == \"YOUR LABEL HERE\"]\n",
    "text = subset.Text.values # list of sentences\n",
    "words  = \" \".join(text) # convert list of sentences into a paragraph of sentences\n",
    "create_wordcloud(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label Encoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "train['label1'] = label_encoder.fit_transform(train['Label'])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train['Text'],\n",
    "                                                    train['Label1'],\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters \n",
    "ngram_range = (1,2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(encoding = 'utf-8',\n",
    "                        ngram_range = ngram_range,\n",
    "                        stop_words = None, \n",
    "                        lowercase = False,\n",
    "                        min_df = min_df,\n",
    "                        max_features = max_features,\n",
    "                        norm = 'l2',\n",
    "                        sublinear_tf = True)\n",
    "\n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "\n",
    "print(features_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(random_state = 1)\n",
    "model.fit(features_train, labels_train)\n",
    "model_predictions = model.predict(features_test)\n",
    "print('Accuracy:',accuracy_score(labels_tests, model_predictions))\n",
    "print(classification_report(labels_test, model_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(labels_test, model_predictions))\n",
    "pd.crosstab(labels_test, model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another Approach\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, confusion_matrix\n",
    "\n",
    "modelx = RandomForestClassifier(n_estimators = 300, \n",
    "                                max_depth = 150,\n",
    "                                n_jobs = 1)\n",
    "\n",
    "modelx.fit(features_train, labels_train)\n",
    "\n",
    "y_pred = modelx.predict(features_test)\n",
    "\n",
    "c_mat = confusion_matrix(y_test,y_pred)\n",
    "kappa = cohen_kappa_score(y_test,y_pred)\n",
    "\n",
    "print(\"Confusion Matrix: \",c_mat)\n",
    "print(\"Kappa: \",kappa)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operations on imbalanced dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_smote, labels_train_smote = smote.fit_sample(features_train.astype(\"float\"), labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print('Before SMOTE: ', Counter(labels_train))\n",
    "print('After SMOTE: ', Counter(labels_train_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(random_state = 1)\n",
    "model.fit(features_train_smote, labels_train_smote)\n",
    "y_predict = model.predict(features_test)\n",
    "\n",
    "print(accuracy_score(labels_test, y_predict))\n",
    "print(classification_report(labels_test, y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(labels_test, y_predict))\n",
    "pd.crosstab(labels_test, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper Parameter Tuning(Random Forest)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "n_estimators = [100, 300, 500, 800, 1200]\n",
    "max_depth = [5, 8, 15, 25, 30]\n",
    "min_samples_split = [2, 5, 10, 15, 100]\n",
    "min_samples_leaf = [1, 2, 5, 10]\n",
    "\n",
    "hyperF = dict(n_estimators = n_estimators, \n",
    "              max_depth = max_depth, \n",
    "              min_samples_split = min_samples_split,\n",
    "              min_samples_leaf = min_samples_leaf)\n",
    "\n",
    "gridF = GridSearchCV(model, hyperF, cv = 3, verbose = 1, n_jobs = -1)\n",
    "\n",
    "bestF = gridF.fit(features_train, labels_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the hyperparameters\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model1 = RandomForestClassifier(random_state = 1, \n",
    "                                max_depth = 30, \n",
    "                                min_samples_leaf = 1,\n",
    "                                min_samples_split = 2,\n",
    "                                n_estimators = 300)\n",
    "\n",
    "model1.fit(features_train, labels_train)\n",
    "\n",
    "model_predictions = model1.predict(features_test)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(labels_test, model_predictions))\n",
    "\n",
    "print(classification_report(labels_test, model_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "model.fit(features_train, labels_train)\n",
    "model_predictions = model.predict(features_test)\n",
    "print('Accuracy: ', accuracy_score(labels_test, model_predictions))\n",
    "print(classification_report(labels_test, model_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ',accuracy_score(labels_test, model_predictions))\n",
    "pd.crosstab(labels_test, model_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassfier()\n",
    "model.fit(features_train_smote, labels_train_smote)\n",
    "y_predict = model.predict(features_test)\n",
    "print(accuracy_score(labels_test, y_predict))\n",
    "print(classification_report(labels_test, y_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(labels_test,y_predict))\n",
    "pd.crosstab(labels_test, y_predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper paramter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "params_KNN = {'n_neighbors':[1,2,3,4,5,6,7], 'p':[1,2,5]}\n",
    "\n",
    "gridF = GridSearchCV(model, params_KNN, cv = 5, verbose = 1, n_jobs = -1)\n",
    "\n",
    "bestF = gridF.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying the best parameter\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors = 7, p = 2)\n",
    "model.fit(features_train, labels_train)\n",
    "model_predictions = model.predict(feature_test)\n",
    "\n",
    "print('Accuracy: ', accuracy_score(labels_test, model_predictions))\n",
    "print(classification_report(labels_test, model_predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(features_train, labels_train)\n",
    "model_predicitions = model.predict(features_test)\n",
    "print('Accuracy: ', accuracy_score(labels_test, model_predictions))\n",
    "print(classification_report(labels_test, model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyper Parametr tuning (multinomial naive bayes)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'alpha':[1, 1e-1, 1e-2]\n",
    "    \n",
    "}\n",
    "\n",
    "gridF = GridSearchCV(model, parameters, cv = 10, verbose = 1, n_jobs = -1)\n",
    "\n",
    "bestF = gridF.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BOW + Xgboost Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidVectorizer, CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer = 'word', token_pattern = r'\\w{1,}')\n",
    "count_vect.fit(train['Text'])\n",
    "\n",
    "train1_trans = count_vect.transform(train['Text'].values)\n",
    "labels = train['Label1'].values\n",
    "\n",
    "X = scipy.sparch.hstack([trian1_trans])\n",
    "y = labels\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(max_depth = 50,\n",
    "                              n_estimators = 80,\n",
    "                              learning_rate = 0.1,\n",
    "                              colsample_bytree = 0.7,\n",
    "                              gamma = 0,\n",
    "                              reg_alpha = 4,\n",
    "                              objective = 'binary:logistic',\n",
    "                              eta = 0.3, \n",
    "                              silent = 1, \n",
    "                              subsample = 0.8).fit(X_train,y_train)\n",
    "\n",
    "                             \n",
    "xgb_prediction = xgb_model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "print('training score:' f1_score(y_train, xgb_model.predict(X_train), average = 'macro'))\n",
    "print('validation score:' f1_score(y_valid, x_model.predict(X_valid),average = 'macro'))\n",
    "print(classification_report(y_valid, xgb_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(y_valid, xgb_prediction))\n",
    "pd.crosstab(y_valid, xgb_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb_model\n",
    "model.fit(features_train_smote, labels_train_smote)\n",
    "y_predict = model.predict(features_test)\n",
    "print('Accuracy:', accuracy_score(labels_test, y_predict))\n",
    "pd.crosstab(labels_test,y_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word level TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer = 'word', ngram_range = (2,3), token_pattern = r'\\w{1,}', max_features = 5000)\n",
    "\n",
    "tfidf_vect.fit(train['Text'])\n",
    "train_trans1 = tfidf_vect.transform(train['Text'].values)\n",
    "labels = train['label1'].values\n",
    "X = scipy.sparse.hstack([train_trans1])\n",
    "y = labels\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(max_depth = 50, \n",
    "                             n_estimators = 80,\n",
    "                             learning_rate = 0.1,\n",
    "                             colsample_bytree = 0.7,\n",
    "                             gamma = 0, \n",
    "                             reg_alpha = 4, \n",
    "                             objective = 'binary:logistic', \n",
    "                             eta = 0.3, \n",
    "                             silent = 1, \n",
    "                             sample = 0.8).fit(X_train,y_train)\n",
    "\n",
    "xgb_predictions =xgb_model.predict(X_valid)\n",
    "\n",
    "print('word level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average = 'macro'))\n",
    "print('word level tf-idf validation score: ', f1_score(y_valid, xgb_model.predict(X_train),average = 'macro'))\n",
    "print(classification_report(y_valid, xgb_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N-gram level TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', \n",
    "                                   token_pattern=r'\\w{1,}',\n",
    "                                   ngram_range=(2,3), \n",
    "                                   max_features=5000)\n",
    "\n",
    "tfidf_vect_ngram.fit(train['Text'])\n",
    "train_trans1 = tfidf_vect_ngram.transform(train['Text'].values)\n",
    "labels = train['Label1'].values\n",
    "X = scipy.sparse.hstack([train_trans1])\n",
    "y = labels\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier(max_depth = 50,\n",
    "                              n_estimators = 80,\n",
    "                              learning_rate = 0.1,\n",
    "                              colsample_bytree = 0.7, \n",
    "                              gamma = 0, \n",
    "                              reg_alpha = 4,\n",
    "                              objective = 'binary:logistic',\n",
    "                              eta = 0.3, \n",
    "                              silent = 1, \n",
    "                              subsample = 0.8).fit(X_train, y_train) \n",
    "xgb_prediction = xgb_model.predict(X_valid)\n",
    "\n",
    "print('n-gram level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
    "print('n-gram level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
    "print(classification_report(y_valid, xgb_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Character level TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char',\n",
    "                                         token_pattern=r'\\w{1,}',\n",
    "                                         ngram_range=(2,3), \n",
    "                                         max_features=5000)\n",
    "\n",
    "tfidf_vect_ngram_chars.fit(train['Text'])\n",
    "train_trans1 = tfidf_vect_ngram_chars.transform(train['Text'].values)\n",
    "labels = train['Label1'].values\n",
    "\n",
    "X = scipy.sparse.hstack([train_trans1])\n",
    "y = labels\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(X,y, test_size = 0.2, random_state = 42)\n",
    "xgb_model = xgb.XGBClassifier(max_depth=50,\n",
    "                              n_estimators =80,\n",
    "                              learning_rate =0.1,\n",
    "                              colsample_bytree= 0.7, \n",
    "                              gamma=0, reg_alpha=4, \n",
    "                              objective='binary:logistic', \n",
    "                              eta=0.3, silent=1,\n",
    "                              subsample=0.8).fit(X_train, y_train) \n",
    "xgb_prediction = xgb_model.predict(X_valid)\n",
    "\n",
    "print('character level tf-idf training score:', f1_score(y_train, xgb_model.predict(X_train), average='macro'))\n",
    "print('character level tf-idf validation score:', f1_score(y_valid, xgb_model.predict(X_valid), average='macro'))\n",
    "print(classification_report(y_valid, xgb_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
